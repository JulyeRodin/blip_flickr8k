# blip_flickr8k
An Image Captioning Project Using the BLIP Model and the Flickr8k Dataset

# Purpose
This is a project to verify the effectiveness of the method proposed in "Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation" (Yue et al., 2023).
We adapted the official implementation to run in a Google Colab environment and conducted experiments to evaluate the proposed method.

This model is developed to compare the performace between basic BLIP model(the model without the SMILE method described in the paper) and SMILE base BLIP model.


# Same condition
The model was developed under the same conditions as the Smile Base model

Used the same hyperparameters as the Smile Base model 

Used the same pretrained BLIPmodel, which is developed by Salesforce

Trained the model with flick8k dataset.

# Result

<img width="996" alt="result" src="https://github.com/user-attachments/assets/ca1743dc-6a96-4a5d-86fe-55906000055e" />


# Reference Link

Paper : [Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation](https://proceedings.neurips.cc/paper_files/paper/2023/file/fa1cfe4e956d85e016b1f8f49b189a0b-Paper-Conference.pdf)

GitHub Repository of the original paper : (https://github.com/yuezih/SMILE)


