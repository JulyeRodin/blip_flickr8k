{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "56a11c7ba05b43a7aeffd5715e4602a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d397b5d15ae040dca07d70fa71314ed9",
              "IPY_MODEL_9ceeb8ada9a845ef930aa36fcfcff657",
              "IPY_MODEL_84abc7ee6d584536948b8576c530cd11"
            ],
            "layout": "IPY_MODEL_2c34c93854704e22b26f8fee7dcb5501"
          }
        },
        "d397b5d15ae040dca07d70fa71314ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7247de4b81564a2bb6c084d6be6a0f94",
            "placeholder": "​",
            "style": "IPY_MODEL_2cc5a56f74e8487884d99a5d34e4aa53",
            "value": "config.json: 100%"
          }
        },
        "9ceeb8ada9a845ef930aa36fcfcff657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed10ad78f62c487d971f3209d2b93304",
            "max": 4563,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c3644650e8c408c916fe228c4da9585",
            "value": 4563
          }
        },
        "84abc7ee6d584536948b8576c530cd11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e990f1224494615997be5e0ff2e91f1",
            "placeholder": "​",
            "style": "IPY_MODEL_23ef0a5959a84089bc137954a75875be",
            "value": " 4.56k/4.56k [00:00&lt;00:00, 285kB/s]"
          }
        },
        "2c34c93854704e22b26f8fee7dcb5501": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7247de4b81564a2bb6c084d6be6a0f94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cc5a56f74e8487884d99a5d34e4aa53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed10ad78f62c487d971f3209d2b93304": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c3644650e8c408c916fe228c4da9585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e990f1224494615997be5e0ff2e91f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23ef0a5959a84089bc137954a75875be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d30b68a20c104802ae229a1f412a3bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf40d970f88948769e1d39f76e8d2f3f",
              "IPY_MODEL_34fa907e740c4636be7e3143379d3ec7",
              "IPY_MODEL_89c8080d9f114e16ac0946fb4a92f693"
            ],
            "layout": "IPY_MODEL_04af0dbcc464460783ece786704dc955"
          }
        },
        "bf40d970f88948769e1d39f76e8d2f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_299558c5f0e54c08a96420bf58c189f1",
            "placeholder": "​",
            "style": "IPY_MODEL_2acf254318f6424092fb9578227d0891",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "34fa907e740c4636be7e3143379d3ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2dbcaf5ff6344edb2c8372c74045e91",
            "max": 989820849,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b0232c6eda64d8a9a40afd25ea446c2",
            "value": 989820849
          }
        },
        "89c8080d9f114e16ac0946fb4a92f693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff348ad549854e18a0e912994dc30985",
            "placeholder": "​",
            "style": "IPY_MODEL_4166b2670778422f97ce1b67378a92b8",
            "value": " 990M/990M [00:07&lt;00:00, 93.9MB/s]"
          }
        },
        "04af0dbcc464460783ece786704dc955": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "299558c5f0e54c08a96420bf58c189f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2acf254318f6424092fb9578227d0891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2dbcaf5ff6344edb2c8372c74045e91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b0232c6eda64d8a9a40afd25ea446c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff348ad549854e18a0e912994dc30985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4166b2670778422f97ce1b67378a92b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQCMzp3liPut",
        "outputId": "3a7d3aa5-7e70-400f-84eb-ea60d8ea113c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2W-6q1hgwCr",
        "outputId": "d571c8ad-a9ef-482e-f880-3d26bc9bbf5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#library import\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from evaluate import load\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n"
      ],
      "metadata": {
        "id": "B9Ls3IvifJdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurations\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 20\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME = \"Salesforce/blip-image-captioning-base\"\n"
      ],
      "metadata": {
        "id": "EU3SCQu1fOdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set zip file path\n",
        "zip_path = '/content/drive/MyDrive/가야 딥러닝2 NLP/가야_final/flickr8k.zip'  # file path of  Google Drive\n",
        "extract_path = '/content/flickr8k/'  # extract file path\n",
        "\n",
        "# extract zip file\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        print(\"Extracting zip file...\")\n",
        "        zip_ref.extractall(extract_path)\n",
        "        print(\"Extraction complete!\")\n",
        "\n",
        "# file path of image and cpation\n",
        "image_folder = os.path.join(extract_path, 'Images')  # a folder containing image data\n",
        "caption_file = os.path.join(extract_path, 'captions.txt')  # caption data file\n",
        "\n",
        "assert os.path.exists(image_folder), \"Image folder not found!\"\n",
        "assert os.path.exists(caption_file), \"Caption file not found!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eebUU6WBjpuW",
        "outputId": "8fce1122-8cf2-40d3-cfbf-80d713a042c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting zip file...\n",
            "Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# caption file load and parsing\n",
        "def load_captions(caption_file):\n",
        "    captions_dict = {}\n",
        "    with open(caption_file, 'r') as f:\n",
        "        lines = f.readlines()[1:]  # first line is a header\n",
        "        for line in lines:\n",
        "            image_name, caption = line.strip().split(',', 1)\n",
        "            caption = caption.strip()\n",
        "            if image_name in captions_dict:\n",
        "                captions_dict[image_name].append(caption)\n",
        "            else:\n",
        "                captions_dict[image_name] = [caption]\n",
        "    return captions_dict\n",
        "\n",
        "captions = load_captions(caption_file)"
      ],
      "metadata": {
        "id": "txDLGjALm8O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# caption check\n",
        "first_3_captions = {}\n",
        "for i, (key, value) in enumerate(captions.items()):\n",
        "    if i == 3:\n",
        "        break\n",
        "    first_3_captions[key] = value\n",
        "\n",
        "print(first_3_captions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyEVyD0dp_2T",
        "outputId": "34da1038-5b7e-46f8-8411-0bd6c2c9a56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1000268201_693b08cb0e.jpg': ['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .'], '1001773457_577c3a7d70.jpg': ['A black dog and a spotted dog are fighting', 'A black dog and a tri-colored dog playing with each other on the road .', 'A black dog and a white dog with brown spots are staring at each other in the street .', 'Two dogs of different breeds looking at each other on the road .', 'Two dogs on pavement moving toward each other .'], '1002674143_1b742ab4b8.jpg': ['A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .', 'A little girl is sitting in front of a large painted rainbow .', 'A small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it .', 'There is a girl with pigtails sitting in front of a rainbow painting .', 'Young girl with pigtails painting outside in the grass .']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset for train\n",
        "class Flickr8kDataset():\n",
        "    def __init__(self, image_folder, captions, processor):\n",
        "        self.image_folder = image_folder\n",
        "        self.captions = captions\n",
        "        self.processor = processor\n",
        "        self.image_names = list(captions.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.image_names[idx]\n",
        "        image_path = os.path.join(self.image_folder, image_name)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        caption = self.captions[image_name][0]  # 첫 번째 캡션 사용\n",
        "\n",
        "        inputs = self.processor(images=image, text=caption, return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True, size=(224, 224))\n",
        "        return {\n",
        "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": inputs[\"input_ids\"].squeeze(0),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "hGKE-6d4mqiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Processor and Dataset\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "dataset = Flickr8kDataset(image_folder=image_folder, captions=captions, processor=processor)\n",
        "\n",
        "# Train-Test-Validation Split\n",
        "train_size = int(0.75 * len(dataset))\n",
        "valid_size = int(0.125 * len(dataset))\n",
        "test_size = len(dataset) - train_size - valid_size\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size], generator=torch.Generator().manual_seed(42))"
      ],
      "metadata": {
        "id": "KNh7VBPq8_tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader\n",
        "batch_size = BATCH_SIZE\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "56a11c7ba05b43a7aeffd5715e4602a0",
            "d397b5d15ae040dca07d70fa71314ed9",
            "9ceeb8ada9a845ef930aa36fcfcff657",
            "84abc7ee6d584536948b8576c530cd11",
            "2c34c93854704e22b26f8fee7dcb5501",
            "7247de4b81564a2bb6c084d6be6a0f94",
            "2cc5a56f74e8487884d99a5d34e4aa53",
            "ed10ad78f62c487d971f3209d2b93304",
            "7c3644650e8c408c916fe228c4da9585",
            "3e990f1224494615997be5e0ff2e91f1",
            "23ef0a5959a84089bc137954a75875be",
            "d30b68a20c104802ae229a1f412a3bb8",
            "bf40d970f88948769e1d39f76e8d2f3f",
            "34fa907e740c4636be7e3143379d3ec7",
            "89c8080d9f114e16ac0946fb4a92f693",
            "04af0dbcc464460783ece786704dc955",
            "299558c5f0e54c08a96420bf58c189f1",
            "2acf254318f6424092fb9578227d0891",
            "b2dbcaf5ff6344edb2c8372c74045e91",
            "3b0232c6eda64d8a9a40afd25ea446c2",
            "ff348ad549854e18a0e912994dc30985",
            "4166b2670778422f97ce1b67378a92b8"
          ]
        },
        "id": "qbPEop-Y9-fQ",
        "outputId": "d377e142-0001-4683-95cc-336812f5e3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56a11c7ba05b43a7aeffd5715e4602a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d30b68a20c104802ae229a1f412a3bb8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train one epoch\n",
        "def train_one_epoch(model, dataloader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels,input_ids = input_ids, attention_mask=attention_mask)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# evaluate\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "\n",
        "            outputs = model(pixel_values=pixel_values, labels=labels,input_ids = input_ids, attention_mask=attention_mask)\n",
        "            total_loss += outputs.loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "a0a4E0oVCm4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_valid_loss = float(\"inf\")  # save the smallest loss\n",
        "\n",
        "# train\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer)\n",
        "    valid_loss = evaluate(model, valid_loader)\n",
        "\n",
        "    # save weight when the loss is the smallest\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), \"best_model_weights.pt\")\n",
        "        print(f\"Model weights saved at epoch {epoch+1} with valid loss: {valid_loss:.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
        "\n",
        "# evaluate test data\n",
        "test_loss = evaluate(model, test_loader)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "ban5f1AiCggC",
        "outputId": "bf549e7f-4751-42b4-8e77-7ff766de29a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 380/380 [07:08<00:00,  1.13s/it]\n",
            "Evaluating: 100%|██████████| 64/64 [00:28<00:00,  2.25it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model weights saved at epoch 1 with valid loss: 1.0184\n",
            "Epoch 1/20, Train Loss: 0.7959, Valid Loss: 1.0184\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 380/380 [07:07<00:00,  1.13s/it]\n",
            "Evaluating: 100%|██████████| 64/64 [00:28<00:00,  2.21it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20, Train Loss: 0.5814, Valid Loss: 1.0773\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 380/380 [07:08<00:00,  1.13s/it]\n",
            "Evaluating: 100%|██████████| 64/64 [00:27<00:00,  2.29it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/20, Train Loss: 0.4213, Valid Loss: 1.1570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 380/380 [07:07<00:00,  1.12s/it]\n",
            "Evaluating: 100%|██████████| 64/64 [00:28<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20, Train Loss: 0.2908, Valid Loss: 1.2488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   3%|▎         | 12/380 [00:14<07:28,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0cf24e2b06df>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-f44d7409baa8>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stopped training because the valid loss showed an increasing trend as the epochs progressed.\n",
        "# it tend to overfit.\n",
        "# first epoch was the best\n",
        "# will use pretrained weight of the first epoch"
      ],
      "metadata": {
        "id": "jyzChRVDjTNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize BLIP model and load weights\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "weights_path = \"/content/best_model_weights.pt\"\n",
        "blip_model.load_state_dict(torch.load(weights_path))  # Load pre-trained weights\n",
        "blip_model = blip_model.to(device)\n",
        "blip_model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Initialize CLIP model and processor\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Initialize BLIP processor\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# Initialize test DataLoader\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Initialize metrics\n",
        "total_caption_length = 0\n",
        "unique_words = set()\n",
        "generated_captions = []\n",
        "true_captions = []\n",
        "total_clip_score = 0\n",
        "total_perplexity = 0\n",
        "image_features_list = []\n",
        "text_features_list = []\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation loop\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        # Prepare inputs\n",
        "        # Get pixel_values (image tensor)\n",
        "\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "\n",
        "        # Generate captions using BLIP model\n",
        "        outputs = blip_model.generate(pixel_values=pixel_values, max_length=32)\n",
        "        generated_caption = blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        true_caption = blip_processor.decode(labels[0], skip_special_tokens=True)\n",
        "\n",
        "        # Calculate Caption Length\n",
        "        total_caption_length += len(generated_caption.split())\n",
        "\n",
        "        # Collect unique words for Lexical Diversity\n",
        "        unique_words.update(generated_caption.split())\n",
        "\n",
        "        # Store captions for later evaluation\n",
        "        generated_captions.append(generated_caption)\n",
        "        true_captions.append(true_caption)\n",
        "\n",
        "\n",
        "\n",
        "        # Generate caption with BLIP model\n",
        "        outputs = blip_model.generate(pixel_values=pixel_values, max_length=32)\n",
        "        generated_caption = blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        true_caption = blip_processor.decode(labels[0], skip_special_tokens=True)\n",
        "\n",
        "        # Compute image features using CLIP\n",
        "        image_features = clip_model.get_image_features(pixel_values)\n",
        "\n",
        "        # Compute text features for generated and true captions\n",
        "        generated_text_inputs = clip_processor(text=[generated_caption], return_tensors=\"pt\", padding=True).to(device)\n",
        "        true_text_inputs = clip_processor(text=[true_caption], return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "        generated_text_features = clip_model.get_text_features(**generated_text_inputs)\n",
        "        true_text_features = clip_model.get_text_features(**true_text_inputs)\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        generated_score = F.cosine_similarity(image_features, generated_text_features).mean().item()\n",
        "        true_score = F.cosine_similarity(image_features, true_text_features).mean().item()\n",
        "\n",
        "        # Average the generated and true scores as the final CLIPScore\n",
        "        clip_score = (generated_score + true_score) / 2\n",
        "        total_clip_score += clip_score\n",
        "\n",
        "        # Compute Perplexity\n",
        "        logits = blip_model(pixel_values=pixel_values, labels=labels, input_ids=input_ids).logits\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(ignore_index=blip_processor.tokenizer.pad_token_id)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "        perplexity = torch.exp(loss)\n",
        "        total_perplexity += perplexity.item()\n",
        "\n",
        "        # Extract image and text embeddings for CLIP self-retrieval\n",
        "        image_features = clip_model.get_image_features(pixel_values)\n",
        "        text_inputs = clip_processor(text=[generated_caption], return_tensors=\"pt\", padding=True).to(device)\n",
        "        text_features = clip_model.get_text_features(**text_inputs)\n",
        "        image_features_list.append(image_features)\n",
        "        text_features_list.append(text_features)\n",
        "\n",
        "# Combine embeddings\n",
        "image_features_tensor = torch.cat(image_features_list, dim=0)\n",
        "text_features_tensor = torch.cat(text_features_list, dim=0)\n",
        "\n",
        "# Compute Recall@1 and Recall@5\n",
        "num_samples = len(test_loader)\n",
        "similarity_scores = torch.matmul(image_features_tensor, text_features_tensor.T)  # Cosine similarity\n",
        "top_k_indices = similarity_scores.topk(5, dim=1).indices\n",
        "r1_count = 0\n",
        "r5_count = 0\n",
        "\n",
        "for i in range(num_samples):\n",
        "    if i in top_k_indices[i][:1]:  # Recall@1\n",
        "        r1_count += 1\n",
        "    if i in top_k_indices[i][:5]:  # Recall@5\n",
        "        r5_count += 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYmAsaV1mg01",
        "outputId": "b38f271b-b216-4468-cd4b-1823833eb6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-4361bf0d9d66>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  blip_model.load_state_dict(torch.load(weights_path))  # Load pre-trained weights\n",
            "Evaluating:   0%|          | 0/1012 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "Evaluating: 100%|██████████| 1012/1012 [10:12<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption Length (Cap. Len.): 11.24\n",
            "Lexical Diversity (Lex. Div.): 0.06\n",
            "Recall@1 (R@1): 0.10\n",
            "Recall@5 (R@5): 0.29\n",
            "CLIPScore: 0.30\n",
            "Perplexity (PPL): 24792.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate final metrics\n",
        "cap_len = total_caption_length / num_samples\n",
        "lexical_diversity = len(unique_words) / total_caption_length\n",
        "recall_at_1 = r1_count / num_samples * 100\n",
        "recall_at_5 = r5_count / num_samples * 100\n",
        "average_clip_score = total_clip_score / num_samples * 100\n",
        "average_ppl = total_perplexity / num_samples\n",
        "\n",
        "# Print results\n",
        "print(f\"Caption Length (Cap. Len.): {cap_len:.2f}\")\n",
        "print(f\"Lexical Diversity (Lex. Div.): {lexical_diversity:.2f}\")\n",
        "print(f\"Recall@1 (R@1): {recall_at_1:.2f}\")\n",
        "print(f\"Recall@5 (R@5): {recall_at_5:.2f}\")\n",
        "print(f\"CLIPScore: {average_clip_score:.2f}\")\n",
        "print(f\"Perplexity (PPL): {average_ppl:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrUdN92q0SJ-",
        "outputId": "a10c8f4d-0e63-449d-b8f8-49f83bec49f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption Length (Cap. Len.): 11.24\n",
            "Lexical Diversity (Lex. Div.): 0.06\n",
            "Recall@1 (R@1): 10.18\n",
            "Recall@5 (R@5): 28.75\n",
            "CLIPScore: 30.27\n",
            "Perplexity (PPL): 24792.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test with real image\n",
        "\n",
        "# Load an image\n",
        "image_path = \"/content/test_image.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")  # Open the image and convert to RGB\n",
        "\n",
        "# Preprocess the image\n",
        "inputs = blip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate caption\n",
        "with torch.no_grad():\n",
        "    outputs = blip_model.generate(**inputs, max_length=32, num_beams=5)\n",
        "    generated_caption = blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Generated Caption: {generated_caption}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEY_LFHw1sGU",
        "outputId": "1490ee61-f19e-4025-d4fb-c564b4f3d12d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Caption: a little girl in a pink shirt holds a rope in front of her face.\n"
          ]
        }
      ]
    }
  ]
}